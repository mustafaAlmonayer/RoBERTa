{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e31680e0-9037-40e9-a635-08c2de8ed9a4",
   "metadata": {},
   "source": [
    "# RoBERTa Fine Tunning For Helpfulness Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a039072-ec9b-428f-b95c-84dce51ac2ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347acc6-4841-4897-96ba-01c43a6b2f26",
   "metadata": {},
   "source": [
    "## Import Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536151d0-6f17-48be-ada0-cb8b528008a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d879199-46bc-42de-8a44-1b307d49ba5e",
   "metadata": {},
   "source": [
    "## Read CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b9715-65b5-43e9-bead-f3818128c473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fcfee4-029c-4d24-be29-66a95fe17f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d1977e-0da4-4760-af37-68c2e57603a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c0bff-2328-477e-87a0-71fc29ca48fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b5cd1-d7b0-4a79-a056-0237c48ce6ab",
   "metadata": {},
   "source": [
    "## Build PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5177a-856a-48ca-9802-ae2218b21b74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063075d7-b8a5-4d04-a348-f40106ffffb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Helpfulness_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, attributes, max_token_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.attributes = attributes\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        text = item.text\n",
    "        attributes = torch.FloatTensor(item[self.attributes])\n",
    "        tokens = self.tokenizer.encode_plus(text,\n",
    "                                            add_special_tokens=True,\n",
    "                                            return_tensors=\"pt\",\n",
    "                                            truncation=True,\n",
    "                                            max_length=self.max_token_len,\n",
    "                                            padding=\"max_length\",\n",
    "                                            return_attention_mask=True)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokens.input_ids.flatten(),\n",
    "            \"attention_mask\": tokens.attention_mask.flatten(),\n",
    "            \"labels\": attributes\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82f2cd-b3b0-4e2a-b23f-4cf55ed3ad16",
   "metadata": {},
   "source": [
    "## Create Train And Test PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c1770f-27c2-4ca9-81ac-b223fd2f5d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributes = [\"helpful\", \"unhelpful\"]\n",
    "model_name = \"roberta-base\"\n",
    "max_token_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "hfs_ds_train = Helpfulness_Dataset(train_df, tokenizer, attributes, max_token_length)\n",
    "hfs_ds_val = Helpfulness_Dataset(test_df, tokenizer, attributes, max_token_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1356846f-49e2-47d8-94d0-cf3b8516f661",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokization Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf5b7e4-455b-4be3-8acd-2a77f9f219e8",
   "metadata": {},
   "source": [
    "### Helpful Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8685dd7c-b51d-4512-bd5e-445b6e69b4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hfs_ds_train.data.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c5233-2411-4f08-8adc-7a04c41cc424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hfs_ds_train.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c6feda-4d33-4db2-8f9e-37acda9a6373",
   "metadata": {},
   "source": [
    "### Unhelpful Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109bfb7-b4c2-4ec7-9252-9ff7320f8c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hfs_ds_val.data.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276225ba-deaa-4dbe-af4f-db1201ac34ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hfs_ds_val.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83a7e49-10a3-49cf-9733-7a19e6fba1ba",
   "metadata": {},
   "source": [
    "# 2. Data Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6f4f47-a59d-4dfb-aab8-04942e3f06aa",
   "metadata": {},
   "source": [
    "## Import Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ee3727-336c-4e28-8755-d4b6c4752f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2cd81-1f90-47ca-a6eb-f3f8bd9186fe",
   "metadata": {},
   "source": [
    "## Creating PyTorch Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f03c3-a698-4b1a-ac8b-cd36c2c92808",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Helpfulness_Data_Model(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, attributes, batch_size, max_token_length, model_name):\n",
    "        super().__init__()\n",
    "        self.attributes = attributes\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_length = max_token_length\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "        if stage in (None, \"fit\"):\n",
    "            self.train_dataset = Helpfulness_Dataset(train_df, tokenizer, attributes, 512)\n",
    "            self.val_dataset = Helpfulness_Dataset(test_df, tokenizer, attributes, 512)\n",
    "        if stage == \"predict\":\n",
    "            self.val_dataset = Helpfulness_Dataset(test_df, tokenizer, attributes, 512)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=False)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=4,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984497c6-c038-42d8-ae4c-4ee0d2fa1fec",
   "metadata": {},
   "source": [
    "## Create Instance Of Our Data Module And Set It Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a96c0c-ed66-4601-a0f1-bbab98f60659",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attributes = [\"helpful\", \"unhelpful\"]\n",
    "model_name = \"roberta-base\"\n",
    "batch_size = 8\n",
    "max_token_length = 512\n",
    "hfs_data_module = Helpfulness_Data_Model(attributes, batch_size, max_token_length, model_name)\n",
    "hfs_data_module.setup()\n",
    "dl = hfs_data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f22d8d-ef56-4dd4-8ea0-18f8a2ebe2f1",
   "metadata": {},
   "source": [
    "## Number Of Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391c37e-2568-4ffb-a6fd-7980ab9f105a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de981efd-d13d-485f-a31f-f6ae2e240d11",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db3f67-c0c0-4f33-9522-6d0140f445b8",
   "metadata": {},
   "source": [
    "## Import Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5c7617-9248-4ed5-8782-545359246f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchmetrics.functional.classification import auroc\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a0c65-da76-4f3d-9c31-83034f18309e",
   "metadata": {},
   "source": [
    "## Helpfulness Classifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2fc45-c56b-4faa-a42c-61845227a5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Helpfulness_Classifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pretrained_model = AutoModel.from_pretrained(config[\"model_name\"], return_dict=True)\n",
    "        self.hidden= nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classification = nn.Linear(self.pretrained_model.config.hidden_size, self.config[\"n_labels\"])\n",
    "        torch.nn.init.xavier_uniform_(self.hidden.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.classification.weight)\n",
    "        self.loss_fun = nn.BCEWithLogitsLoss(reduction=\"mean\")\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        output = self.pretrained_model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        pooled_output = torch.mean(output.last_hidden_state, 1)\n",
    "        pooled_output = self.hidden(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output = F.relu(pooled_output)\n",
    "        logits = self.classification(pooled_output)\n",
    "        loss = 0 \n",
    "        if labels is not None:\n",
    "            loss = self.loss_fun(logits.view(-1, self.config[\"n_labels\"]), labels.view(-1, self.config[\"n_labels\"]))\n",
    "        return loss, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        loss, logits = self(**batch)\n",
    "        self.log(\"train loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": logits, \"labels\": batch[\"labels\"]}\n",
    "    \n",
    "    def validations_step(self, batch, batch_index):\n",
    "        loss, logits = self(**batch)\n",
    "        self.log(\"validation loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"val_loss\": loss, \"predictions\": logits, \"labels\": batch[\"labels\"]}\n",
    "    \n",
    "    def predict_step(self, batch, batch_index):\n",
    "        _, logits = self(**batch)\n",
    "        return logits\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config[\"lr\"], weight_decay=self.config[\"w_decay\"])\n",
    "        total_steps = self.config[\"train_size\"] / self.config[\"bs\"]\n",
    "        warmup_steps = math.floor(total_steps * self.config[\"warmup\"])\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        return [optimizer], [scheduler]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cab09f8-f1e0-4127-9b07-8c5cc525a800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": \"roberta-base\",\n",
    "    \"n_labels\": len(attributes),\n",
    "    \"bs\": 8,\n",
    "    \"lr\": 2e-5,\n",
    "    \"warmup\": 0.2,\n",
    "    \"w_decay\": 0.001,\n",
    "    \"train_size\": len(hfs_data_module.train_dataloader()),\n",
    "    \"n_epochs\": 4\n",
    "}\n",
    "\n",
    "model = Helpfulness_Classifier(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c52b577-c722-42d8-b1b7-bdcc1d521726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "input_ids = hfs_ds_train.__getitem__(idx)[\"input_ids\"]\n",
    "am = hfs_ds_train.__getitem__(idx)[\"attention_mask\"]\n",
    "labels = hfs_ds_train.__getitem__(idx)[\"labels\"]\n",
    "\n",
    "loss, output = model(input_ids.unsqueeze(dim=0), am.unsqueeze(dim=0), labels.unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71ea88-54ca-46f5-bf25-781a10789e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013059ef-ea95-4313-951f-d7fdd6028766",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d1f4b-7df2-4a5e-89c6-ccda54a9e48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium')\n",
    "hfs_data_module = Helpfulness_Data_Model(attributes, config[\"bs\"], max_token_length, model_name)\n",
    "hfs_data_module.setup()\n",
    "\n",
    "model = Helpfulness_Classifier(config)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=config[\"n_epochs\"], num_sanity_val_steps=50)\n",
    "trainer.fit(model, hfs_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83acaf-9b2c-4a5c-8304-feed0a33a090",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Predict / Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf163e-c875-4569-8011-11e635dd4d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_reviews(model, dm):\n",
    "    preictions = trainer.predict(model, datamodule=dm)\n",
    "    flattened_prediction = np.stack([torch.sigmoid(torch.Tensor(p)) for batch in preictions for p in batch])\n",
    "    return flattened_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b598e90-7277-4e23-a387-a289b8238aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hfs_data_module.val_dataset.data[[\"helpful\", \"helpful\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958bea8f-4b1e-4599-a810-a6f07f230111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = classify_reviews(model, hfs_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98472f99-3394-4458-8781-471a614443cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helpful_array = hfs_data_module.val_dataset.data[[\"helpful\", \"unhelpful\", \"_id\"]]\n",
    "helpful_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428c5ed-194d-4d34-8c38-d4791befad39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b31b22-2834-4b22-a73e-7b73e11f5de6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_confusion_matrix(predictions, ground_truth_labels):\n",
    "    \n",
    "    tp, fp, tn, fn, low_confidence = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        helpful = 1 if predictions.iloc[i][0] >= 0.5 else 0\n",
    "        unhelpful = 1 if predictions.iloc[i][1] >= 0.5 else 0\n",
    "        if helpful == 1 and helpful == ground_truth_labels.iloc[i][\"helpful\"]:\n",
    "            tp += 1 \n",
    "        elif helpful == 1 and helpful != ground_truth_labels.iloc[i][\"helpful\"]:\n",
    "            fp += 1\n",
    "        elif unhelpful == 1 and unhelpful == ground_truth_labels.iloc[i][\"unhelpful\"]:\n",
    "            tn += 1\n",
    "        elif unhelpful == 1 and unhelpful != ground_truth_labels.iloc[i][\"unhelpful\"]:\n",
    "            fn += 1\n",
    "        else:\n",
    "            low_confidence += 1\n",
    "            \n",
    "    return tp, fp, tn, fn, low_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813000d-7819-436a-b66c-ffdba30d3ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tp, fp, tn, fn, low_confidence = generate_confusion_matrix(pd.DataFrame(predictions), hfs_data_module.val_dataset.data[[\"helpful\", \"unhelpful\"]])\n",
    "print(f\"True Positive = {tp}\")\n",
    "print(f\"False Positive = {fp}\")\n",
    "print(f\"True negative = {tn}\")\n",
    "print(f\"False negative = {fn}\")\n",
    "print(f\"Low Confidence = {low_confidence}\")\n",
    "print(f\"Total = {tp + fp + tn + fn + low_confidence}\")\n",
    "with open(\"Results/RoBERTa-Base-Met.txt\", \"w\") as f:\n",
    "    f.write(f\"True Positive = {tp}\\n\")\n",
    "    f.write(f\"False Positive = {fp}\\n\")\n",
    "    f.write(f\"True negative = {tn}\\n\")\n",
    "    f.write(f\"False negative = {fn}\\n\")\n",
    "    f.write(f\"Low Confidence = {low_confidence}\")\n",
    "    f.write(f\"Total = {tp + fp + tn + fn + low_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebe0ca-9073-4503-a062-f1826af920fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(tp, fp, tn, fn):\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) != 0 else 0.0\n",
    "    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) != 0 else 0.0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0.0\n",
    "    sensitivity = recall  # Sensitivity is another name for Recall\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) != 0 else 0.0\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score, specificity, sensitivity, false_positive_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c28dff-37d3-408e-9387-a65f756d756e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1_score, specificity, sensitivity, false_positive_rate = calculate_metrics(tp, fp, tn, fn)\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Precision = {precision}\")\n",
    "print(f\"Recall = {recall}\")\n",
    "print(f\"F1 Score = {f1_score}\")\n",
    "print(f\"Specificity = {specificity}\")\n",
    "print(f\"Sensitivity = {sensitivity}\")\n",
    "print(f\"False Positive Rate = {false_positive_rate}\")\n",
    "with open(\"Results/RoBERTa-Base-Results.txt\", \"w\") as f:\n",
    "    f.write(f\"Accuracy = {accuracy}\\n\")\n",
    "    f.write(f\"Precision = {precision}\\n\")\n",
    "    f.write(f\"Recall = {recall}\\n\")\n",
    "    f.write(f\"F1 Score = {f1_score}\\n\")\n",
    "    f.write(f\"Specificity = {specificity}\\n\")\n",
    "    f.write(f\"Sensitivity = {sensitivity}\\n\")\n",
    "    f.write(f\"False Positive Rate = {false_positive_rate}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
